from transformers import AutoModelForCausalLM, AutoTokenizer

# ì„¤ì¹˜ëª…ë ¹ì–´ pip install torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121
import torch

# ë””ë°”ì´ìŠ¤ í™•ì¸
device = "cuda" if torch.cuda.is_available() else "cpu"
print("CUDA available:", torch.cuda.is_available())
if device == "cuda":
    free_mem, total_mem = torch.cuda.mem_get_info()
    print(f"GPU memory: {free_mem / 1024**3:.2f} GB free / {total_mem / 1024**3:.2f} GB total")

# Zephyr ëª¨ë¸ë¡œ ë³€ê²½
model_name = "HuggingFaceH4/zephyr-7b-alpha"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(user_input: str) -> str:
    # ZephyrëŠ” ChatML í¬ë§·ì„ ì‚¬ìš©
    messages = [
        {"role": "system", "content": "You are a helpful and friendly AI assistant."},
        {"role": "user", "content": user_input},
    ]

    # Chat í…œí”Œë¦¿ ì ìš©
    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # ì‘ë‹µ ìƒì„±
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

    # ì „ì²´ ì‘ë‹µ ë””ì½”ë”© í›„, assistant ë‹µë³€ë§Œ ì¶”ì¶œ
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    reply = decoded.split(messages[-1]["content"])[-1].strip()

    print("âœ… Response ready.")
    return reply

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    while True:
        user_text = input("ğŸ‘¤ User: ")
        if user_text.lower() in ["exit", "quit"]:
            break
        reply = generate_response(user_text)
        print("ğŸ¤– Zephyr:", reply)
